# LLM Hub Environment Configuration
# Copy this file to .env and adjust values as needed

# ==========================================
# LM Studio Configuration
# ==========================================
# URL to LM Studio API endpoint
# For Docker Desktop on Windows/Mac, use host.docker.internal
# For native Docker on Linux, use your host IP address
LM_STUDIO_URL=http://host.docker.internal:1234

# ==========================================
# Bridge Service Configuration
# ==========================================
# Port for LM Studio Bridge service (internal)
MCP_PORT=3000

# Interval in seconds for model discovery polling
# Lower values = more frequent checks, higher CPU usage
POLL_INTERVAL=30

# ==========================================
# Gateway Configuration
# ==========================================
# Port for unified gateway service (public)
# This is the main entry point for API requests
GATEWAY_PORT=8080

# API key for bearer token authentication
# Change this to a secure random string in production
# Generate with: openssl rand -hex 32
API_KEY=your-api-key-here

# Maximum requests per minute per API key
# Adjust based on your usage requirements
RATE_LIMIT_PER_MINUTE=60

# Enable/disable gateway authentication
# Set to false for development, true for production
AUTH_ENABLED=true

# ==========================================
# Logging Configuration
# ==========================================
# Log level for all services
# DEBUG: Verbose logging for development
# INFO: Standard operational logging
# WARNING: Only warnings and errors
# ERROR: Only errors
LOG_LEVEL=INFO
