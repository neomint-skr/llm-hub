"""
"Show Me!" Test Framework for LLM Hub
User-friendly testing that demonstrates working functionality with clear visual feedback
"""

import asyncio
import time
import json
import sys
from typing import Dict, Any, List, Optional
from pathlib import Path
import subprocess
import requests
from dataclasses import dataclass
from enum import Enum

# Add project root to path
project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root))

# Import visual indicators
from visual_indicators import VisualIndicators, IndicatorStyle


class TestStatus(Enum):
    """Test execution status"""
    PENDING = "â³"
    RUNNING = "ðŸ”„"
    PASSED = "âœ…"
    FAILED = "âŒ"
    SKIPPED = "â­ï¸"


@dataclass
class TestResult:
    """Individual test result"""
    name: str
    status: TestStatus
    duration: float
    message: str
    details: Optional[Dict[str, Any]] = None


class ShowMeTestFramework:
    """User-friendly test framework with enhanced visual feedback"""

    def __init__(self, visual_style: IndicatorStyle = IndicatorStyle.STANDARD):
        self.results: List[TestResult] = []
        self.start_time = 0
        self.gateway_url = "http://localhost:8080"
        self.bridge_url = "http://localhost:3000"
        self.lm_studio_url = "http://localhost:1234"

        # Initialize visual indicators
        self.indicators = VisualIndicators(visual_style)

    def print_header(self):
        """Print test framework header with enhanced visuals"""
        self.indicators.print_test_header(
            "LLM Hub 'Show Me!' Test Framework",
            total_tests=9  # Update this when adding more tests
        )
        self.indicators.print_info_box(
            "What this does",
            [
                "Tests all major LLM Hub components",
                "Shows clear visual feedback for each test",
                "Provides specific solutions for any issues",
                "Demonstrates that everything is working correctly"
            ]
        )

    def print_test_start(self, test_name: str, step: int = 0):
        """Print test start message with enhanced visuals"""
        self.indicators.print_test_start(test_name, step)

    def print_test_result(self, result: TestResult):
        """Print individual test result with enhanced visuals"""
        # Map our TestStatus to indicator status strings
        status_map = {
            TestStatus.PASSED: 'passed',
            TestStatus.FAILED: 'failed',
            TestStatus.RUNNING: 'running',
            TestStatus.PENDING: 'pending',
            TestStatus.SKIPPED: 'skipped'
        }

        status_str = status_map.get(result.status, 'unknown')
        self.indicators.print_test_result(
            result.name,
            status_str,
            result.duration,
            result.message,
            result.details
        )

    async def run_test(self, test_name: str, test_func, step: int = 0, *args, **kwargs) -> TestResult:
        """Run a single test with timing and enhanced visual feedback"""
        self.print_test_start(test_name, step)
        start_time = time.time()

        try:
            result = await test_func(*args, **kwargs) if asyncio.iscoroutinefunction(test_func) else test_func(*args, **kwargs)
            duration = time.time() - start_time

            if isinstance(result, tuple):
                success, message, details = result
            elif isinstance(result, bool):
                success, message, details = result, "Test completed", None
            else:
                success, message, details = True, str(result), None

            test_result = TestResult(
                name=test_name,
                status=TestStatus.PASSED if success else TestStatus.FAILED,
                duration=duration,
                message=message,
                details=details
            )

        except Exception as e:
            duration = time.time() - start_time
            test_result = TestResult(
                name=test_name,
                status=TestStatus.FAILED,
                duration=duration,
                message=f"Error: {str(e)}",
                details={"exception_type": type(e).__name__}
            )

        self.results.append(test_result)
        self.print_test_result(test_result)
        return test_result

    def test_docker_services(self) -> tuple:
        """Test: Docker services are running"""
        try:
            # Check if Docker is available
            result = subprocess.run(["docker", "version"], capture_output=True, text=True, timeout=10)
            if result.returncode != 0:
                return False, "Docker is not running", {"docker_available": False}

            # Check if our services are running
            result = subprocess.run(
                ["docker", "ps", "--filter", "name=llm-hub", "--format", "{{.Names}}"],
                capture_output=True, text=True, timeout=10
            )

            running_services = result.stdout.strip().split('\n') if result.stdout.strip() else []
            expected_services = ["llm-hub-gateway", "llm-hub-bridge"]

            services_running = len([s for s in running_services if any(exp in s for exp in expected_services)])

            return (
                services_running >= 2,
                f"Found {services_running} LLM Hub services running",
                {"running_services": running_services, "services_count": services_running}
            )

        except subprocess.TimeoutExpired:
            return False, "Docker command timed out", None
        except FileNotFoundError:
            return False, "Docker not installed", None

    def test_gateway_health(self) -> tuple:
        """Test: Gateway health endpoint responds"""
        try:
            response = requests.get(f"{self.gateway_url}/health", timeout=10)

            if response.status_code == 200:
                health_data = response.json()
                return (
                    True,
                    f"Gateway healthy - Status: {health_data.get('status', 'unknown')}",
                    {
                        "status_code": response.status_code,
                        "response_time": response.elapsed.total_seconds(),
                        "health_status": health_data.get('status'),
                        "service_count": health_data.get('services', {}).get('total', 0)
                    }
                )
            else:
                return False, f"Gateway returned HTTP {response.status_code}", {"status_code": response.status_code}

        except requests.exceptions.ConnectionError:
            return False, "Cannot connect to Gateway", {"url": self.gateway_url}
        except requests.exceptions.Timeout:
            return False, "Gateway health check timed out", None

    def test_bridge_health(self) -> tuple:
        """Test: Bridge health endpoint responds"""
        try:
            response = requests.get(f"{self.bridge_url}/health", timeout=10)

            if response.status_code == 200:
                health_data = response.json()
                return (
                    True,
                    f"Bridge healthy - Service: {health_data.get('service', 'unknown')}",
                    {
                        "status_code": response.status_code,
                        "response_time": response.elapsed.total_seconds(),
                        "service": health_data.get('service'),
                        "version": health_data.get('version')
                    }
                )
            else:
                return False, f"Bridge returned HTTP {response.status_code}", {"status_code": response.status_code}

        except requests.exceptions.ConnectionError:
            return False, "Cannot connect to Bridge", {"url": self.bridge_url}
        except requests.exceptions.Timeout:
            return False, "Bridge health check timed out", None

    def test_lm_studio_connection(self) -> tuple:
        """Test: LM Studio is accessible"""
        try:
            response = requests.get(f"{self.lm_studio_url}/v1/models", timeout=10)

            if response.status_code == 200:
                models_data = response.json()
                model_count = len(models_data.get('data', []))
                return (
                    True,
                    f"LM Studio connected - {model_count} models available",
                    {
                        "status_code": response.status_code,
                        "model_count": model_count,
                        "response_time": response.elapsed.total_seconds()
                    }
                )
            else:
                return False, f"LM Studio returned HTTP {response.status_code}", {"status_code": response.status_code}

        except requests.exceptions.ConnectionError:
            return False, "LM Studio not running or not accessible", {"url": self.lm_studio_url}
        except requests.exceptions.Timeout:
            return False, "LM Studio connection timed out", None

    def test_mcp_tools_available(self) -> tuple:
        """Test: MCP tools are discoverable"""
        try:
            response = requests.get(f"{self.gateway_url}/mcp/tools", timeout=10)

            if response.status_code == 200:
                tools_data = response.json()
                tool_count = len(tools_data.get('tools', []))
                tool_names = [tool.get('name', 'unnamed') for tool in tools_data.get('tools', [])]

                return (
                    tool_count > 0,
                    f"Found {tool_count} MCP tools: {', '.join(tool_names[:3])}{'...' if len(tool_names) > 3 else ''}",
                    {
                        "tool_count": tool_count,
                        "tool_names": tool_names,
                        "response_time": response.elapsed.total_seconds()
                    }
                )
            else:
                return False, f"MCP tools endpoint returned HTTP {response.status_code}", {"status_code": response.status_code}

        except requests.exceptions.ConnectionError:
            return False, "Cannot connect to MCP tools endpoint", None
        except requests.exceptions.Timeout:
            return False, "MCP tools request timed out", None

    def test_end_to_end_inference(self) -> tuple:
        """Test: End-to-end inference works"""
        try:
            # First get available tools
            tools_response = requests.get(f"{self.gateway_url}/mcp/tools", timeout=10)
            if tools_response.status_code != 200:
                return False, "Cannot get MCP tools for inference test", None

            tools_data = tools_response.json()
            inference_tools = [tool for tool in tools_data.get('tools', []) if 'inference' in tool.get('name', '').lower()]

            if not inference_tools:
                return False, "No inference tools available", {"available_tools": len(tools_data.get('tools', []))}

            # Try to call inference (this would be a mock call in real implementation)
            test_prompt = "Hello, this is a test prompt"

            # For now, just verify the tool is available
            return (
                True,
                f"Inference tool available: {inference_tools[0].get('name')}",
                {
                    "inference_tools_count": len(inference_tools),
                    "test_prompt_length": len(test_prompt),
                    "tool_name": inference_tools[0].get('name')
                }
            )

        except Exception as e:
            return False, f"Inference test failed: {str(e)}", None

    def print_summary(self):
        """Print test execution summary with enhanced visuals"""
        total_tests = len(self.results)
        passed_tests = len([r for r in self.results if r.status == TestStatus.PASSED])
        failed_tests = len([r for r in self.results if r.status == TestStatus.FAILED])
        total_duration = sum(r.duration for r in self.results)
        success_rate = (passed_tests/total_tests*100) if total_tests > 0 else 0

        # Use enhanced visual summary
        self.indicators.print_summary(
            total_tests, passed_tests, failed_tests, total_duration, success_rate
        )

        # Add success celebration if all tests passed
        if failed_tests == 0:
            self.indicators.print_success_celebration(
                "All tests passed! LLM Hub is working correctly."
            )
            self.indicators.print_info_box(
                "Your LLM Hub is ready to use",
                [
                    "Gateway: http://localhost:8080",
                    "Health Dashboard: http://localhost:8080/health",
                    "Control Center: http://localhost:9000",
                    "Predictive Dashboard: units/health-monitor/dashboard.html"
                ]
            )
        else:
            self.indicators.print_info_box(
                "Common solutions",
                [
                    "Make sure Docker Desktop is running",
                    "Run start.bat to start LLM Hub services",
                    "Check that LM Studio is running on port 1234",
                    "Run setup-autostart.bat for automatic startup",
                    "Run start-control-center.bat for the control interface"
                ]
            )

    def test_autostart_configured(self) -> tuple:
        """Test: Autostart is properly configured"""
        try:
            # Check if autostart task exists in Windows Task Scheduler
            result = subprocess.run(
                ["schtasks", "/query", "/tn", "LLM Hub Autostart"],
                capture_output=True, text=True, timeout=10
            )

            if result.returncode == 0:
                return True, "Autostart is configured in Task Scheduler", {"task_exists": True}
            else:
                return False, "Autostart not configured - run setup-autostart.bat", {"task_exists": False}

        except subprocess.TimeoutExpired:
            return False, "Task Scheduler query timed out", None
        except FileNotFoundError:
            return False, "Task Scheduler not available (not Windows?)", None

    def test_control_center_available(self) -> tuple:
        """Test: Control Center is accessible"""
        try:
            response = requests.get("http://localhost:9000", timeout=5)

            if response.status_code == 200:
                return (
                    True,
                    "Control Center is accessible",
                    {
                        "status_code": response.status_code,
                        "response_time": response.elapsed.total_seconds()
                    }
                )
            else:
                return False, f"Control Center returned HTTP {response.status_code}", {"status_code": response.status_code}

        except requests.exceptions.ConnectionError:
            return False, "Control Center not running - start with start-control-center.bat", {"url": "http://localhost:9000"}
        except requests.exceptions.Timeout:
            return False, "Control Center connection timed out", None

    def test_predictive_maintenance_active(self) -> tuple:
        """Test: Predictive maintenance is monitoring"""
        try:
            response = requests.get(f"{self.bridge_url}/health", timeout=10)

            if response.status_code == 200:
                health_data = response.json()
                predictions = health_data.get('predictions', {})
                monitoring_active = predictions.get('monitoring_active', False)

                if monitoring_active:
                    data_points = predictions.get('data_points', {})
                    total_samples = sum(data_points.values()) if data_points else 0

                    return (
                        True,
                        f"Predictive maintenance active - {total_samples} data points collected",
                        {
                            "monitoring_active": monitoring_active,
                            "total_samples": total_samples,
                            "data_points": data_points
                        }
                    )
                else:
                    return False, "Predictive maintenance not active", {"monitoring_active": False}
            else:
                return False, "Cannot check predictive maintenance status", {"status_code": response.status_code}

        except Exception as e:
            return False, f"Predictive maintenance check failed: {str(e)}", None

    async def run_all_tests(self):
        """Run all 'Show Me!' tests with enhanced visual feedback"""
        self.print_header()
        self.start_time = time.time()

        # Core infrastructure tests
        self.indicators.print_section_header("Core Infrastructure Tests")
        await self.run_test("Docker Services Running", self.test_docker_services, 1)
        await self.run_test("Gateway Health Check", self.test_gateway_health, 2)
        await self.run_test("Bridge Health Check", self.test_bridge_health, 3)

        # Integration tests
        self.indicators.print_section_header("Integration Tests")
        await self.run_test("LM Studio Connection", self.test_lm_studio_connection, 4)
        await self.run_test("MCP Tools Discovery", self.test_mcp_tools_available, 5)
        await self.run_test("End-to-End Inference", self.test_end_to_end_inference, 6)

        # Advanced features tests
        self.indicators.print_section_header("Advanced Features Tests")
        await self.run_test("Autostart Configuration", self.test_autostart_configured, 7)
        await self.run_test("Control Center Available", self.test_control_center_available, 8)
        await self.run_test("Predictive Maintenance Active", self.test_predictive_maintenance_active, 9)

        self.print_summary()

        # Cleanup visual indicators
        self.indicators.cleanup()

        # Return success status
        failed_count = len([r for r in self.results if r.status == TestStatus.FAILED])
        return failed_count == 0


async def main():
    """Main entry point"""
    framework = ShowMeTestFramework()
    success = await framework.run_all_tests()

    sys.exit(0 if success else 1)


if __name__ == "__main__":
    asyncio.run(main())
